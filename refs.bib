@article{class_imbalance,
title = {An empirical comparison of techniques for the class imbalance problem in churn prediction},
journal = {Information Sciences},
volume = {408},
pages = {84-99},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517306618},
author = {Bing Zhu and Bart Baesens and Seppe K.L.M. {vanden Broucke}},
keywords = {Churn prediction, Class imbalance, Benchmark experiment, Expected maximum profit measure},
abstract = {Class imbalance brings significant challenges to customer churn prediction. Many solutions have been developed to address this issue. In this paper, we comprehensively compare the performance of state-of-the-art techniques to deal with class imbalance in the context of churn prediction. A recently developed expected maximum profit criterion is used as one of the main performance measures to offer more insights from the perspective of cost-benefit. The experimental results show that the applied evaluation metric has a great impact on the performance of techniques. An in-depth exploration of reaction patterns to different measures is conducted by intra-family comparison within each solution group and global comparison among the representative techniques from different groups. The results also indicate there is much space to improve solutionsâ€™ performance in terms of profit-based measure. Our study offers valuable insights for academics and professionals and it also provides a baseline to develop new methods for dealing with class imbalance in churn prediction.}
}
@article{smote,
title = {SMOTE: Synthetic Minority Over-sampling Technique},
journal = {Journal of Artificial Intelligence Research},
volume = {321-357},
year = {2002},
doi = {https://doi.org/10.1613/jair.953},
url = {https://www.jair.org/index.php/jair/article/view/10302},
author = {N. V. Chawla and K. W. Bowyer and L. O. Hall and W. P. Kegelmeyer},
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ``normal'' examples with only a small percentage of ``abnormal'' or ``interesting'' examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.}
}