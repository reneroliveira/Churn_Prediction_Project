
\documentclass[12pt,letterpaper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[
backend=bibtex,
sorting=ynt
]{biblatex}
\addbibresource{refs.bib}

\newtheorem{defi}{Definition}
\newtheorem{teo}{Theorem}
\newtheorem{prop}{Proposition}
\hypersetup{%
	colorlinks=true,
	linkcolor=blue,
	linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{array}
\usepackage{multirow}
\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}
\newcommand\course{Rener Oliveira}
\newcommand\lcur{\mathcal{L}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\rr}{\mathbb{R}^2}
\newcommand{\rn}{\mathbb{R}^n}
\newcommand{\linesep}{{\color{black} \rule{\linewidth}{0.5mm} }}
\newcommand{\rpos}{\mathbb{R}_{>0}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}
\newcommand{\bd}[1]{\boldsymbol{#1}}
\newcommand{\gt}{>}%broken keyboard
\newcommand{\pow}{^}%broken keyboard
\newcommand{\pr}{\operatorname{Pr}} %% probability
\newcommand{\vr}{\operatorname{Var}} %% variance
\newcommand{\rs}{X_1, X_2, \ldots, X_p} %%  random sample
\newcommand{\irs}{X_1, X_2, \ldots} %% infinite random sample
\newcommand{\rsd}{x_1, x_2, \ldots, x_p} %%  random sample, realised
\newcommand{\Sm}{\bar{X}_n} %%  sample mean, random variable
\newcommand{\sm}{\bar{x}_n} %%  sample mean, realised
\newcommand{\Sv}{\bar{S}^2_n} %%  sample variance, random variable
\newcommand{\sv}{\bar{s}^2_n} %%  sample variance, realised
\newcommand{\bX}{\boldsymbol{X}} %%  random sample, contracted form (bold)
\newcommand{\bx}{\boldsymbol{x}} %%  random sample, realised, contracted form (bold)
\newcommand{\bT}{\boldsymbol{T}} %%  Statistic, vector form (bold)
\newcommand{\bt}{\boldsymbol{t}} %%  Statistic, realised, vector form (bold)
\newcommand{\emv}{\hat{\theta}_{\text{EMV}}}
\newcommand{\defn}{\stackrel{\textrm{\scriptsize def}}{=}}
\newcommand{\op}{\operatorname}
\newcommand{\eps}{\varepsilon}
\newcommand{\norm}{\mathcal{N}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\iid}{\overset{\text{iid}}{\sim}}
\pagestyle{fancyplain}
\headheight 35pt        
\chead{\textbf{\Large Churn Prediction Project}}
\lhead{Machine Learning\\EMAp FGV}
\rhead{\small{\course \\ \today}}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em
\usepackage{xcolor}
\definecolor{maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{halfgray}{gray}{0.55}
\definecolor{ipython_frame}{RGB}{207, 207, 207}
\definecolor{ipython_bg}{RGB}{247, 247, 247}
\definecolor{ipython_red}{RGB}{186, 33, 33}
\definecolor{ipython_green}{RGB}{0, 128, 0}
\definecolor{ipython_cyan}{RGB}{64, 128, 128}
\definecolor{ipython_purple}{RGB}{170, 34, 255}

\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{ %
	language=R,                     % the language of the code
	basicstyle=\footnotesize,       % the size of the fonts that are used for the code
	numbers=left,                   % where to put the line-numbers
	numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
	stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
	% will be numbered
	numbersep=5pt,                  % how far the line-numbers are from the code
	backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	showtabs=false,                 % show tabs within strings adding particular underscores
	frame=single,                   % adds a frame around the code
	rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
	tabsize=2,                      % sets default tabsize to 2 spaces
	captionpos=b,                   % sets the caption-position to bottom
	breaklines=true,                % sets automatic line breaking
	breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
	title=\lstname,                 % show the filename of files included with \lstinputlisting;
	% also try caption instead of title
	keywordstyle=\color{blue},      % keyword style
	commentstyle=\color{dkgreen},   % comment style
	stringstyle=\color{mauve},      % string literal style
	%escapeinside={\%*}{*)},         % if you want to add a comment within your code
	morekeywords={*,...}            % if you want to add more keywords to the set
} 
\lstset{
	breaklines=true,
	%
	extendedchars=true,
	literate=
	{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
	{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
	{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
	{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
	{ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
	{Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
	{â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
	{Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
	{œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
	{ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
	{€}{{\EUR}}1 {£}{{\pounds}}1
}

%%
%% Python definition (c) 1998 Michael Weber
%% Additional definitions (2013) Alexis Dimitriadis
%% modified by me (should not have empty lines)
%%

\begin{document}
	\tableofcontents
	\newpage
	\section{Introduction}
	
	Customer churn (loss of customers) is a problem for telecommunication companies, considering an environment of increasing competition. When a company loses customers, it not only loses the future revenue, but also the investment made to get those customers. According to \cite{class_imbalance}, some studies show that acquiring new clients is five to six times more expensive than retaining existing ones. 
	
	Telecom companies have two approaches to deal with churners: the reactive one, which tries to convince customers who wants to cancel to stay; and the proactive one, which predicts who is more likely to churn before they explicitly decide to churn and send them suitable offers to avoid their loss.
	
	The aim of this project is to study the churn problem using the \href{https://www.crowdanalytix.com/contests/why-customer-churn/}{CrowdAnalytix} dataset, which has information regarding usage patterns from customers of a telecom company, which the real name was anonymized. This data was part of a Machine Learning Challenge issued by CrowdAnalytix in 2012.
	
	The raw data contains 3333 observations of 20 variables which are described in Table \ref{features}. The \href{https://www.kaggle.com/mnassrib/telecom-churn-datasets}{Kaggle} copy of the data was already divided into 80\% train and 20\% test. All code is on this \href{https://github.com/reneroliveira/Churn_Prediction_Project}{Github}.
	
	\begin{table}[!htb]
		\centering
		\caption{Features Description}
		\label{features}
		\begin{tabular}{|c|c|} \hline
			\textbf{Variable} & \textbf{Description}\\  \hline 
			 State  &  Customer's state \\ \hline
			 Account.length& Time since subscription \\ \hline
			 Area.code   & Phone number area code\\ \hline 
			 International.plan& Has an international plan? (Yes=1,No=0)\\ \hline 
			 Voice.mail.plan    & Has a voicemail plan? (Yes=1,No=0)\\ \hline
			 Number.vmail.messages& Number of voicemail messages\\ \hline
			 Total.day.minutes& Total minutes used during the day\\ \hline
			 Total.day.calls& Total calls made during days\\ \hline
			 Total.day.charge& Total charge during days\\ \hline
			 Total.eve.minutes& Total minutes used during evenings\\ \hline
			 Total.eve.calls& Total calls made during evenings\\ \hline
			 Total.eve.charge& Total charge during evenings\\ \hline 
			 Total.night.minutes & Total minutes used during nights\\ \hline 
			 Total.night.calls& Total calls made during nights\\ \hline
			 Total.night.charge& Total charge during nights\\ \hline 
			 Total.intl.minutes& Total international minutes used\\ \hline 
			 Total.intl.calls& Total international calls made\\ \hline 
			 Total.intl.charge& Total international charge\\\hline 
			 Customer.service.calls& Number of calls to customer services\\ \hline 
			 Churn & Has the customer churned? (Yes=1,No=0)\\ \hline
		\end{tabular}
	\end{table}
	
	\section{Data Cleaning and EDA}
	
	The State variable was transformed into regions (Northeast, South, North Central, and West), reducing the number of dummy variables from 50 to just 4. We also removed four columns from the data, which can be explained by the Figure \ref{corr} continuous variables correlation plot\footnote{We generated all plots in this section from the training set.}. 
	
	\begin{figure}[!htb]
		\centering
		\caption{Continuous variables correlation}
		\label{corr}
		\includegraphics[scale=1]{images/corr.png}
	\end{figure}
	
	As we can see above, the vast majority of the features are uncorrelated. But the pairs like total minutes/total charge are perfectly correlated, which makes sense if users are charged by minutes used. We choose to remove the charge variables to avoid multicollinearity.
	
	Visualizing the distribution of some variables we can find interesting patterns. In Figure \ref{voicemail} we see that among churners, we have fewer customers with voice mail plan (approximately 16.75\%) comparing with non-churners subscribed to this plan (approximately 29.32\%). On the other hand, we have much more churners with an international plan, comparing with non-churners, as shown in Figure \ref{international}. A possible explanation would be that international plan subscribers are finding better offers in this service with competitors.
	
	\begin{figure}[!htb]
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[scale=0.7]{images/voicemail.png}
		\caption{Voicemail clients proportion}
		\label{voicemail}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[scale=0.7]{images/international.png}
		\caption{International plan clients proportion}
		\label{international}
		\end{subfigure}
	\caption{Categorical Variables}
	\end{figure}

	Another insightful plot is the one in Figure \ref{cus_services}. We can see that churners make more calls to customer services than non-churners; these calls may be related to complaints about the services and unsatisfactory manifestations.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.685]{images/customer_services.png}
		\caption{Customer services calls distribution}
		\label{cus_services}
	\end{figure}

Taking a look at the regional distribution in Figure \ref{region}, although Northeast and South regions have more churners, the churn percentage in all of them is around 14.6\% which is the percentage in the training set as a whole.
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.685]{images/region.png}
		\caption{Churners distribution by region}
		\label{region}
	\end{figure}

About the percentage of churners, an approximate proportion of 14\% in the training and test set (see Figure \ref{imbalance}) might be a problem for classifier algorithms because the sample will bias the learning models to the majority class (non-churners). These models may behave poorly in the prediction power of the minority class (churners). In the next section, we discuss some ways to handle this potential problem.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.7]{images/imbalance.png}
	\caption{Churners percentage by dataset}
	\label{imbalance}
\end{figure}
	\newpage
	\section{Classification Methods}
	\subsection{Evaluation Metrics}
	The task of churn prediction is a binary classification problem, i.e we have a $n\times p$ features matrix $\bd X$, and a target column vector $\bd y \in \{0,1\}^{n}$, which represents churners as stated in the introduction. We want to learn functions  that receive an $p$-dimensional feature input and outputs a binary value, 0 or 1.
	
	With a learned function/model in hands we can apply it in our test set and build the Confusion Matrix which summarizes how the model has performed.
	
	%https://tex.stackexchange.com/questions/20267/how-to-construct-a-confusion-matrix-in-latex
	\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{table}[!htb]
	\centering
	\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
		\multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ Value}} & 
		& \multicolumn{2}{c}{\bfseries Model Outcome} & \\
		& & \bfseries 1 & \bfseries 0 \\
		& 1 & \MyBox{(TP) True}{Positive} & \MyBox{(FN) False}{Negative}  \\[2.4em]
		& 0 & \MyBox{(FP) False}{Positive} & \MyBox{(TN) True}{Negative}  \\
	\end{tabular}
\caption{Confusion Matrix}
\end{table}

From this table we can derive the following metrics:
\begin{itemize}
	\item Accuracy: (TP+TN)/N;
	\item Recall: TP/(TP+FN),
\end{itemize}
where N is the total number of test observations (TP+TN+FP+FN). Accuracy measures what proportion of the test set our model predicted correctly. When dealing with class imbalance, a model which assigns the majority class to everyone will have high accuracy but will perform poorly in predicting positive instances (churners).

The goal of our prediction algorithm is to correctly identify churners to proactively prevent customer loss. So, in this case, a False Negative has much more impact than a False positive, because the company wants to predict as many churners as possible before they churn. A False Negative may lead to a customer loss that would be avoidable if predicted correctly, but targeting an offer/discount to a False Positive that wouldn't churn is not too harmful, because this client will stay loyal to the company.

Recall aims to measure prediction power among positive instances, i.e the proportion of actual churners we're capturing by the algorithm.

We're also gonna use the \textbf{AUC-ROC} metric, a number between 0 and 1 with measures the area under ROC curve, which is generated plotting false positive rate (FP/(FP+TN)) against true positive rate (TP/(TP+FN)) using different probability/odds thresholds outputted from models. A random classifier will have an AUC equal to $0.5$.

Another important metric, widely used in literature and industry is the \textbf{lift}\cite{class_imbalance}. If the model yields probabilities, we sort the test predictions by these probabilities in a descending order, then we define the lift as the ratio between the percentage of positive entries in the top 10\% lines ($\beta_ {10\%}$), and the percentage of churners (positives) in the entire test set ($\beta_0$). We then define the \textbf{top-decile-lift} as the fraction:
$$\dfrac{\beta_{10\%}}{\beta_0}$$

We can use other percentile values instead of 10\%, but we'll use the decile version since it's widely used performance metric for this kind of problem. A top-decile lift of 2 means that the model classifies two times more churners in the top 10\% group than a random classifier. The lower bound is 0, but a non-random classifier should have a lift at least greater than 1. The upper bound occurs when $\beta_ {10\%} = 100\%$, so in our case with $\beta_0\approx 14.2\%$, the lift upper bound will be $\approx 7.042254$. Such a maximum lift means that all entries in the top decile are churners.

We’ll devote special attention to recall and top-decile lift and recall because of the imbalance problem. If a model predicts 0 to every test observation, it would be approximately 85\% accurate, but recall would be zero, and lift undefined.
 
	
	\subsection{Resampling methods}
	The reference\cite{class_imbalance} presents a lot of data-level solutions to deal with class imbalance. The goal is to resample our training set to create a rebalanced new one. We'll focus on the three techniques mentioned below:
	
	\begin{itemize}
		\item (ROS) Random Oversampling: randomly replicates churners instances;
		\item (RUS) Random Undersampling: randomly eliminates the non-churners instances;
		\item (SMOTE) Synthetic minority oversampling technique\cite{smote}
	\end{itemize}
	
	With undersampling, we might be removing valuable information from the dataset. Oversampling may cause biased observations since we're going to replicate minority points several times to rebalance the data. 
	
	Smote oversamples the minority class in a smarter way, creating synthetic new data. To do that, for each existing (churner) observation SMOTE selects a random point out of the k-nearest-neighbors (we'll use the default k=5) and creates a new point by performing a random linear interpolation between the point in consideration and the selected neighbor. The article \cite{smote} shows details of this algorithm. We'll use the smote R implementation of performanceEstimation package\cite{performanceestimation}. For ROS and RUS we created our own code, available in the \href{https://github.com/reneroliveira/Churn_Prediction_Project/blob/main/functions.R}{functions.R GitHub file}. 
	
	\subsection{Classification Algorithms and Results}
	\textbf{K-Nearest-Neighbors}
	
	KNN may be the simplest classification algorithm that predicts the class by the majority of the $K$ nearest neighbors classes of the point in consideration. The probability of a given observation belongs to the class $j$ ($j=0,1$) is the proportion of neighbors of the class $j$.
	
	We performed 5-fold cross-validation and selected $K$ that maximized the sensitivity (recall), which was $K=8$. Then, we standardized all variables and trained the model using resampled training sets (by RUS, ROS, and SMOTE) and calculated performance metrics in the test set. The results are in Table \ref{knn}.
	
	% latex table generated in R 3.6.1 by xtable 1.8-4 package
	% Wed Jun 23 19:41:43 2021
	\begin{table}[!htb]
		\centering
		\addtolength{\tabcolsep}{6pt}    
		\begin{tabular}{|r|cccc|}
			\hline
			& Accuracy & Recall & Lift & AUC \\ 
			\hline
			KNN  & \textbf{0.8711} & 0.1789 & 0.5240 & 0.1963 \\ 
			KNN + ROS & 0.6987 & 0.6842 & 0.8383 & 0.4056 \\ 
			KNN + RUS & 0.8066 & 0.6211 & 0.6288 & 0.4448 \\ 
			KNN + SMOTE & 0.7196 & \textbf{0.7368} & \textbf{1.1527} & \textbf{0.5124} \\ 
			\hline
		\end{tabular}
	\caption{KNN results}
	\label{knn}
	\end{table}

 The first line labeled as just "KNN" stands for the metrics obtained training the model directly in the imbalanced original dataset. We bolded the best value of each metric. Although the original model got a high accuracy, it got a pretty bad recall (0.1789), which makes sense since we don't have many positive instances in the training set for the model learn by neighbors. In general, KNN performed badly, but the smoted dataset yielded acceptable metrics (AUC>0.5, lift>1), and it's better than a random classifier.
 
 \textbf{Logistic Regression}
 
 Logistic Regression models the log-odds of being positive (churner) as a linear function of the predictors.
 $$\log\left(\dfrac{p(y_i=1)}{1-p(y_i=1)}\right)=\beta_0+\sum_{j=1}^{p}\beta_jX_{ij}$$
 We estimate the coefficients by maximizing the likelihood of a binomial distribution model for the target.
 As before, we trained this model using the balanced resampled training sets and calculated the performance in the test set. Results follows in Table \ref{logit}.
 
 % latex table generated in R 3.6.1 by xtable 1.8-4 package
 % Wed Jun 23 19:41:43 2021
 \begin{table}[!htb]
 	\centering
 	\addtolength{\tabcolsep}{6pt}
 	\begin{tabular}{|c|cccc|}
 		\hline
 		& Accuracy & Recall & Lift & AUC \\ 
 		\hline
 		LogisticReg  & \textbf{0.8546} & 0.1789 & 3.1438 & 0.8244 \\ 
 		LogisticReg + ROS & 0.7661 & 0.7684 & 3.1438 & \textbf{0.8303} \\ 
 		LogisticReg + RUS & 0.7631 & 0.7368 & 3.1438 & 0.8213 \\ 
 		LogisticReg + SMOTE & 0.7391 &\textbf{ 0.8211} &\textbf{ 3.3533} & 0.8292 \\ 
 		\hline
 	\end{tabular}
 \caption{Logistic Regression results}
 \label{logit}
 \end{table}
 
Again, training in the imbalanced data yielded a poor recall. Although we lost accuracy with resampling, we got substantially higher recall which is a crucial metric to this problem, as we stated before. 

Let’s review the coefficients of SMOTE version of the regression, which was the best one in terms of recall and lift. We got more statistically significant features than in the model trained in the original data. In both cases the “strongest” predictor (in terms of z-value) was Customer.service.calls, followed by International.plan and Total.day.minutes.
 \begin{lstlisting}
 	## Coefficients:
 	##                          Estimate Std. Error z value Pr(>|z|)    
 	## (Intercept)            -7.2338220  0.6458328 -11.201  < 2e-16 ***
 	## Account.length          0.0021405  0.0009932   2.155  0.03116 *  
 	## Area.code              -0.0006780  0.0008921  -0.760  0.44727    
 	## International.plan      2.5649186  0.1242593  20.642  < 2e-16 ***
 	## Voice.mail.plan        -2.1678067  0.3801978  -5.702 1.19e-08 ***
 	## Number.vmail.messages   0.0452635  0.0120397   3.760  0.00017 ***
 	## Total.day.minutes       0.0133785  0.0007205  18.568  < 2e-16 ***
 	## Total.day.calls         0.0051677  0.0019800   2.610  0.00905 ** 
 	## Total.eve.minutes       0.0072255  0.0008227   8.783  < 2e-16 ***
 	## Total.eve.calls        -0.0008711  0.0020065  -0.434  0.66420    
 	## Total.night.minutes     0.0032962  0.0008402   3.923 8.74e-05 ***
 	## Total.night.calls       0.0031070  0.0020616   1.507  0.13178    
 	## Total.intl.minutes      0.0767535  0.0146975   5.222 1.77e-07 ***
 	## Total.intl.calls       -0.0983638  0.0176142  -5.584 2.35e-08 ***
 	## Customer.service.calls  0.6687630  0.0298297  22.419  < 2e-16 ***
 	## RegionNortheast         0.4678842  0.1194881   3.916 9.01e-05 ***
 	## RegionSouth             0.2496615  0.1024976   2.436  0.01486 *  
 	## RegionWest              0.0861546  0.1143103   0.754  0.45103    
 	## ---
 	## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 \end{lstlisting}

\textbf{Bagging \& Random Forest}
	
	Bagging multiples decision trees in the dataset selecting different bootstrapped samples each time. The final prediction is given by the majority vote aggregating the results of the trained trees. Random Forest is like bagging, but each time a new split is made in a decision tree, a random subset of $m$ predictor out of $p$ are selected, and the split uses only these variables. We can draw a probabilistic output by taking the proportion of positive/negative votes among the trees. 
	
For bagging, we used R's default hyperparameter of 500 trees. Again, we trained the model in the different training sets and compared performance in the test set. See results in Table \ref{bag1}.

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Wed Jun 23 20:18:23 2021
\begin{table}[!htb]
	\centering
	\addtolength{\tabcolsep}{6pt}
	\begin{tabular}{|r|cccc|}
		\hline
		& Accuracy & Recall & Lift & AUC \\ 
		\hline
		Bagging  & \textbf{0.9610} & 0.7789 & \textbf{6.9163} & \textbf{0.9303} \\ 
		Bagging + ROS & 0.9505 & 0.7579 &\textbf{ 6.9163} & 0.9192 \\ 
		Bagging + RUS & 0.9025 & \textbf{0.8632} & 5.8683 & 0.9165 \\ 
		Bagging + SMOTE & 0.9265 & 0.8526 & 6.4971 & 0.9247 \\ 
		\hline
	\end{tabular}
\caption{Bagging Results}
\label{bag1}
\end{table}

Good results in general, but RUS and SMOTE yielded better recall without much prejudice to other metrics, specially SMOTE, with a higher lift than RUS. Bagging trained in the original dataset was the best, except for recall, our key focus. Nevertheless, it produced a 0.7789 measure which is not too bad compared to what we have seen with logistic regression and KNN.

For Random Forest, we selected the value of $m=6$, which optimized OOB error. We also did 5-fold cross-validation to choose the number of trees, but all tested values gave similar results, so we used the default value of 500. Results follow in Table \ref{rf}. Again, resampled training sets enhanced recall, but the original dataset got a higher lift, very close to the upper bound of 7.0422.

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Wed Jun 23 20:29:44 2021
\begin{table}[!htb]
	\centering
	\addtolength{\tabcolsep}{6pt}
	\begin{tabular}{|r|cccc|}
		\hline
		& Accuracy & Recall & Lift & AUC \\ 
		\hline
		Random Forest  & 0.9565 & 0.7368 & \textbf{7.0211} & 0.9281 \\ 
		Random Forest + ROS &\textbf{ 0.9655} & 0.8105 & 6.7067 & 0.9216 \\ 
		Random Forest + RUS & 0.8936 & \textbf{0.8526} & 5.9731 & 0.9197 \\ 
		Random Forest + SMOTE & 0.9325 & 0.8421 & 6.3923 & \textbf{0.9298} \\ 
		\hline
	\end{tabular}
\caption{Random Forest results}
\label{rf}
\end{table}

\textbf{Bagging-based ensembles}
The way we're overcoming imbalanced in the training set until now is by the data-level solution of resampling it to rebalance it. From now on, we'll focus on algorithm-level solutions, which use the original dataset and deal with imbalance internally. This part focuses on modifications in bagging in the bootstrap step; In the next part, we present a modified Adaboost which deals with imbalance by weighting a minority misclassification.

Bagging-based ensembles as proposed in \cite{ensembles} are original bagging, but instead of bootstrapping the entire dataset, for each bag, one uses RUS, ROS, or SMOTE to create a balanced sample. This produces brand new models that we can call RUSBagging, ROSBagging, and SMOTEBagging, depending on the technique used. We used R implementations from \cite{iric}. Results (comparing with original bagging) are shown in Table \ref{bag2}.

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Wed Jun 23 20:48:38 2021
\begin{table}[!htb]
	\centering
	\addtolength{\tabcolsep}{6pt}
	\begin{tabular}{|r|cccc|}
		\hline
		& Accuracy & Recall & Lift & AUC \\ 
		\hline
		Bagging  & \textbf{0.9610} & 0.7789 & \textbf{6.9163} & \textbf{0.9303} \\ 
		RUSBagging & 0.8906 & \textbf{0.8632} & 6.2875 & 0.9176 \\ 
		ROSBagging & 0.8831 & \textbf{0.8632} & 6.6019 & 0.9070 \\ 
		SMOTEBagging & 0.8846 & 0.8526 & 5.6588 & 0.9052 \\ 
		\hline
	\end{tabular}
\caption{Bagged-based ensembles results}
\label{bag2}
\end{table}

All 3 methods yielded better recall measures than original bagging, but ROSBagging got a higher lift among them (although original bagging was better). The other metrics were pretty close among the 3 models.

\textbf{AdaBoost and Cost-sensitive learning}

AdaBoost\cite{ada} is a boosting algorithm for classification problems. It uses weak learners to fit the data and then iteratively uses the error to retrain the model and improve the overall learner. As the final set of algorithms, we used the implementation of \cite{iric} of the modified version of AdaBoost proposed in \cite{ada} to deal with class imbalance. The goal is to introduce a higher cost for misclassifying the minority class (churners). The article proposes three different modified formulas with costs (AdaC1, AdaC2, and AdaC3), but we just tested the AdaC2, which was the only one implemented in IRIc package\cite{iric}.

To adjust the cost parameter we created a validation set using 1/8 of the training set with the same churn proportion. Figure \ref{adacost} shows the Recall/Accuracy measures for different costs values. This cost will penalize the algorithm for misclassifying a churner, so if the weight is too high we're gonna end with a model that only yield positive classes and has a poor overall accuracy in spite of recall close to 1.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.85]{images/adacost.png}
	\caption{Cost parameter tuning}
	\label{adacost}
\end{figure}
Although we argued that Recall is a crucial measure in our case, we don’t want a low accuracy, so we choose 1.4 as the cost, which seems to be a good equilibrium point in the validation set.

Then we applied the original AdaBoost, and compared to the AdaC2, the version with higher cost to false negatives. The results are shown in Table \ref{adab}.

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Wed Jun 23 21:14:25 2021
\begin{table}[ht]
	\centering
	\addtolength{\tabcolsep}{6pt}
	\begin{tabular}{|r|cccc|}
		\hline
		& Accuracy & Recall & Lift & AUC \\ 
		\hline
		AdaBoost &\textbf{ 0.9505} & 0.7158 & \textbf{6.7067} & \textbf{0.9270} \\ 
		AdaC2 & 0.8816 & \textbf{0.8316} & 6.4971 & 0.9257 \\ 
		\hline
	\end{tabular}
\caption{Boosting results}
\label{adab}
\end{table}
Original Adaboost performed better in all metrics except Recall. Losing some accuracy, we can get a better recall using AdaC2, without too much prejudice to the other measures.

\section{Final Results and Comments}
Table \ref{whole} shows the metrics for all models tested in this project. 
Although we have these champions for each metric, excluding KNN and the basic Logistic Regression all the models performed very well. A recall lower bound of 0.71 (AdaBoost) means we're identifying 71\% of the real churners. 

A lift lower bound of 3.1 (Logistic + RUS/ROS) means we have 3.1 times more churners in the top decile than a random classifier would have. This is approximately 44\%, but excluding logistic, all lifts were higher than 5, which corresponds to around 71\% of churners in the top decile. Our lift champion, Random Forest almost reached the maximum lift, so using these models to target the top decile with a suitable offer would be a good idea to avoid churn.

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Wed Jun 23 21:18:09 2021
\begin{table}[!htb]
	\centering
	\addtolength{\tabcolsep}{6pt}
	\begin{tabular}{|r|cccc|}
		\hline
		& Accuracy & Recall & Lift & AUC \\ 
		\hline
		LogisticReg  & 0.8546 & 0.1789 & 3.1438 & 0.8244 \\ 
		LogisticReg + ROS & 0.7661 & 0.7684 & 3.1438 & 0.8303 \\ 
		LogisticReg + RUS & 0.7631 & 0.7368 & 3.1438 & 0.8213 \\ 
		LogisticReg + SMOTE & 0.7391 & 0.8211 & 3.3533 & 0.8292 \\ 
		KNN  & 0.8711 & 0.1789 & 0.5240 & 0.1963 \\ 
		KNN + ROS & 0.6987 & 0.6842 & 0.8383 & 0.4056 \\ 
		KNN + RUS & 0.8066 & 0.6211 & 0.6288 & 0.4448 \\ 
		KNN + SMOTE & 0.7196 & 0.7368 & 1.1527 & 0.5124 \\ 
		Bagging  & 0.9610 & 0.7789 & 6.9163 & \textbf{0.9303} \\ 
		Bagging + ROS & 0.9505 & 0.7579 & 6.9163 & 0.9192 \\ 
		Bagging + RUS & 0.9025 & \textbf{0.8632} & 5.8683 & 0.9165 \\ 
		Bagging + SMOTE & 0.9265 & 0.8526 & 6.4971 & 0.9247 \\ 
		RForest  & 0.9565 & 0.7368 &\textbf{ 7.0211} & 0.9281 \\ 
		RForest + ROS & \textbf{0.9655} & 0.8105 & 6.7067 & 0.9216 \\ 
		RForest + RUS & 0.8936 & 0.8526 & 5.9731 & 0.9197 \\ 
		RForest + SMOTE & 0.9325 & 0.8421 & 6.3923 & 0.9298 \\ 
		RUSBagging & 0.8906 & \textbf{0.8632} & 6.2875 & 0.9176 \\ 
		ROSBagging & 0.8831 & \textbf{0.8632} & 6.6019 & 0.9070 \\ 
		SMOTEBagging & 0.8846 & 0.8526 & 5.6588 & 0.9052 \\ 
		AdaBoost & 0.9505 & 0.7158 & 6.7067 & 0.9270 \\ 
		AdaC2 & 0.8816 & 0.8316 & 6.4971 & 0.9257 \\ 
		\hline
	\end{tabular}
\caption{Final Results}
\label{whole}
\end{table}


	\newpage
	\addcontentsline{toc}{section}{References}
	%			\bibliography{refs}
	\printbibliography
\end{document}